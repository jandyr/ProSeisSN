{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeMSvOAPfX5S"
   },
   "source": [
    "### ---\n",
    "# Unit13: Machine Learning\n",
    "This notebook has the activities of the Course **ProSeisSN**. It deals with time series processing using a passive seismic dataset using [ObsPy](https://docs.obspy.org/).\n",
    "\n",
    "#### Dependencies: Obspy, Numpy, Matplotlib\n",
    "#### Reset the Jupyter notebook in order to run it again, press:\n",
    "***Kernel*** -> ***Restart & Clear Output***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "uzMJ-lGlkGuK",
    "outputId": "66c27f9e-fd21-4a54-ed6d-6a23b77ec540",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------ Import Libraries and widgets\n",
    "import sys\n",
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "#------ Import PyTorch Library\n",
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn \n",
    "print('PyTorch version ==>', torch.__version__)\n",
    "#------ Work with the directory structure to include auxiliary codes\n",
    "print('\\n Local directory ==> ', os.getcwd())\n",
    "print('  - Contents: ', os.listdir(), '\\n')\n",
    "\n",
    "path = os.path.abspath(os.path.join('..'))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path+\"/CodePy\")\n",
    "\n",
    "%run ../CodePy/ImpMod.ipynb\n",
    "\n",
    "#------ Alter default matplotlib rcParams\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.dates as dates\n",
    "# Change the defaults of the runtime configuration settings in the global variable matplotlib.rcParams\n",
    "plt.rcParams['figure.figsize'] = 9, 5\n",
    "#plt.rcParams['lines.linewidth'] = 0.5\n",
    "plt.rcParams[\"figure.subplot.hspace\"] = (.9)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "#------ widgets\n",
    "output = widgets.Output()\n",
    "#------ Magic commands\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "    \n",
    "#%pylab notebook\n",
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Basics of PyTorch\n",
    "### Tensors\n",
    "- Tensors are a specialized data structure that are very similar to arrays and matrices. PyTorch uses tensors to encode the inputs and outputs of a model, as well as the model’s parameters. Tensors can run on GPUs or other hardware accelerators.\n",
    "\n",
    "- A comprehensive list of tensor operations is found in [here](https://pytorch.org/docs/stable/torch.html 'Tensors').\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./nvmt.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "- The **stress tensor** are the forces applied to the material,\n",
    "$$\\underline{\\boldsymbol{\\sigma}}=\\left(\\begin{array}{ccc}\n",
    "\t\t\t\\sigma_{11} & \\sigma_{12} & \\sigma_{13}\\\\\n",
    "\t\t\t\\sigma_{21} & \\sigma_{22} & \\sigma_{23}\\\\\n",
    "\t\t\t\\sigma_{31} & \\sigma_{32} & \\sigma_{33}\\\\\n",
    "\t\t\\end{array}\\right) $$\n",
    "\n",
    "- The **strain symmetric tensor** holds the spatial derivatives of the **displacement field** deformations under the stress,\n",
    "$$\\underline{\\mathbf{e}}=\\left(\\begin{array}{ccc}\n",
    "\tu_{11} & \\frac{1}{2}\\left(u_{12}+u_{21}\\right) & \\frac{1}{2}\\left(u_{13}+u_{31}\\right)\\\\\n",
    "\t\\frac{1}{2}\\left(u_{21}+u_{12}\\right) & u_{22} & \\frac{1}{2}\\left(u_{23}+u_{32}\\right)\\\\\n",
    "\t\\frac{1}{2}\\left(u_{31}+u_{13}\\right) & \\frac{1}{2}\\left(u_{32}+u_{23}\\right)& u_{33}\n",
    "\\end{array}\\right) $$\n",
    "\n",
    "- The relation between stress and strain are given by the Hooke's law,\n",
    "\n",
    "  $$\\underline{\\boldsymbol{\\sigma}}=\\underset{=}{\\mathbf{c}}\\underline{\\mathbf{e}},$$\n",
    "\n",
    "  $$\\underset{=}{\\mathbf{c}}=\\left(\\begin{array}{cccccc}\n",
    "\t\t\\lambda\\!+\\!2\\mu & \\lambda & \\lambda & 0 & 0 & 0\\\\\n",
    "\t\t\\lambda & \\lambda\\!+\\!2\\mu & \\lambda & 0 & 0 & 0\\\\\n",
    "\t\t\\lambda & \\lambda & \\lambda\\!+\\!2\\mu & 0 & 0 & 0\\\\\n",
    "\t\t0 & 0 & 0 & \\mu & 0 & 0\\\\\n",
    "\t\t0 & 0 & 0 & 0 & \\mu & 0\\\\\n",
    "\t\t0 & 0 & 0 & 0 & 0 & \\mu\n",
    "\t\\end{array}\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = lambda: input(\"Press Enter to continue): \")\n",
    "#------ Create from lists\n",
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "#\n",
    "#------ Create from NumPy ndarray\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "#\n",
    "#------ Create\n",
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------ Attributes\n",
    "print(f\"\\nShape of rand_tensor: {rand_tensor.shape}\")\n",
    "print(f\"Datatype of ones_tensor: {ones_tensor.dtype}\")\n",
    "print(f\"zeros_tensor is stored on: {zeros_tensor.device}\")\n",
    "#\n",
    "#------ Slicing\n",
    "print(f\"\\nFirst row: {rand_tensor[0]}\")\n",
    "print(f\"First column: {rand_tensor[:, 0]}\")\n",
    "print(f\"Last column: {rand_tensor[..., -1]}\")\n",
    "rand_tensor[:,1] = 0\n",
    "print(f\"Sliced random Tensor: \\n {rand_tensor} \\n\")\n",
    "w()\n",
    "#\n",
    "#------ Concatenate tensors along a given dimension. Operations.\n",
    "rand_tensor = torch.cat([rand_tensor, rand_tensor, rand_tensor], dim=1)\n",
    "print(f\"\\nConcatenated random Tensor: \\n {rand_tensor} \\n\")\n",
    "#\n",
    "dummy = rand_tensor.sum()\n",
    "rand_tensor.add(10)\n",
    "print(f\"Add 10 in place to random Tensor: {rand_tensor} \")\n",
    "#\n",
    "#------ Bridges with numpy\n",
    "n = rand_tensor.numpy()\n",
    "print(f\"\\nrand_tensor to nupy: \\n {n} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A PyTorch model expects the input data to be in float32 by default. A numpy array is float64 (double) by default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#------ operations\n",
    "dummy = np.random.rand(3, 4)\n",
    "rand_tensor = torch.from_numpy(dummy)\n",
    "print(f\"\\nNumpy random \\n{dummy}\\n as a Tensor: \\n {rand_tensor} \\n\")\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = rand_tensor @ rand_tensor.T\n",
    "y2 = rand_tensor.matmul(rand_tensor.T)\n",
    "print(f\"\\nIs y1 = y2? \\n{y1 == y2}\")\n",
    "# Define y3 first\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(rand_tensor, rand_tensor.T, out=y3)\n",
    "print(f\"\\nIs y1 = y3? \\n{y1 == y3}\")\n",
    "#\n",
    "#------ Move tensor to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    rand_tensor = rand_tensor.to(\"cuda\")\n",
    "print(f\"\\nrand_tensor is stored on: {zeros_tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Machine Learning\n",
    "Machine learning is a specific area of artificial intelligence for computer systems to learn using various statistical models and mathematical techniques. In this way machines can extract meaningful information from data by recognizing patterns.\n",
    "1) In the context of **ML** the data samples are **features**, which should good descriptors of the model.\n",
    "- If data samples have associated output labels then it is supervised learning.\n",
    "- If data is unlabeled, it is unsupervised learning.\n",
    "\n",
    "2) A model learns a function which maps an I/P $\\mathbf{X}$ onto an O/P $\\mathbf{Y}$: $\\mathbf{F}\\left(\\mathbf{X}\\right)\\mapsto\\mathbf{Y}$.\n",
    "\n",
    "3) The parameters of a linear model, $\\mathcal{W}_{0},\\mathcal{W}_{i}$, the bias and the intercepts, give wieghts to the data, or **features** $\\mathbf{X}_{i}$,\n",
    "$$\\mathbf{Y}=\\mathcal{W}_{0}+\\sum_{i}\\mathcal{W}_{i}\\mathbf{X}_{i}$$\n",
    "\n",
    "4) Those parameters are *learned* minimizing a loss function. An example in the L2-norm is the Mean Squared Error\n",
    "$$\\textrm{MSE}=\\frac{1}{n}\\sum_{i}^{n}\\left(\\mathbf{Y}_{i}-\\hat{\\mathbf{Y}}_{i}\\right)^{2},$$\n",
    "where $\\mathbf{Y}_{i}$ is the observed *label values* and $\\hat{\\mathbf{Y}* their predicted values.\n",
    "| Machine Learning|\n",
    "| :- | \n",
    "| • Regression |\n",
    "| • Logistic Regression |\n",
    "| • Support Vector Machines |\n",
    "| ••• |\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./MLwrkf.png\" width=\"800\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Workflow\n",
    "| ML workflow|\n",
    "| :- | \n",
    "| Gather data $\\mathbf{X}$, labels $\\mathbf{Y}$, define a model $\\mathbf{M}\\left(\\mathcal{W}\\right)$ and loss function $L\\left(\\mathbf{X}\\mathbf{Y};\\mathcal{W}\\right)$ |\n",
    "| Initialize model parameters $\\mathcal{W}$ |\n",
    "| Calculate the loss function for the $\\mathcal{W}$ |\n",
    "| Update parameters to minimize $L\\left(\\mathbf{X},\\mathbf{Y};\\mathcal{W}\\right)$ |\n",
    "| Repeat until convergence|\n",
    "\n",
    "1) Minimize $L\\left(\\mathbf{X},\\mathbf{Y};\\mathcal{W}\\right)\\,\\rightarrow$ **Gradient Descent**. The local minima reduce the loss function; the global minimum may not be reachable.\n",
    "\n",
    "2) Learn the distribution of the training data by minimizing the loss on it.\n",
    "- A model can only predict well on new data if them follows the training distribution. **EG** a model trained with the TTB data won’t work well for data from a glacier calving.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./ttbkgi.png\" width=\"1000\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Neural Networks\n",
    "1) **Neural Networks** are universal function approximators; they can learn any function. They can be trained end$\\leftrightarrow$end.\n",
    "\n",
    "2) **Deep learning** is a type of machine learning that focuses on learning hierarchical representations of data. Deep learning algorithms, particularly **artificial neural networks** (ANNs), consist of interconnected nodes, neurons or units, organized into layers, each node is connected to every node in next layer, *i.e.*, they are fully connected, thus a **Fully Connected Network** (FCN).\n",
    "\n",
    "- Each neuron in a layer is connected to every neuron in the subsequent layer, each connection having an associated weight determining the strength of the connection.\n",
    "- The basic ANN consists of an input layer, followed by one or more hidden layers, connected in the end to an output layer. The weights are adjusted iteratively through optimization algorithms, such as the gradient descent.\n",
    "\n",
    "3) The cost function $L$ represents the relationship between the predicted output of the model $\\hat{\\mathbf{Y}}$ and the actual output $\\mathbf{Y}$. We minimize the cost or loss function of a model using the **gradient descent** technique. In ML functions are often more\n",
    "complex than a simple convex function, with multiple local minima and maxima. Various optimization techniques have been devised to address this complexity efficiently such as such as momentum, adaptive learning rates and variants of gradient descent, *e.g.*, stochastic gradient descent.\n",
    "\n",
    "4) **Convolutional Neural Networks** (CNNs), are a type of deep neural network, which is exceptionally efficient at processing and analyzing visual data, images and videos. They are highly effective in image classification, object detection, and image segmentation, turning them ideal in tasks that require an understanding of the spatial structure of the input data.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./ann.png\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earthquake arrival time based on station location and earthquake location\n",
    "- The predicted arrival time is, \n",
    "$$t_p^i = T^i\\left(x_i,y_i,z_i,x_s,y_s,z_s, V_H\\right)=\\left(\\mathbf{X}_i,\\mathbf{X}_s, V_H\\right)$$\n",
    "where $\\vec{X}_i=[x_i,y_i,z_i]$ is the station location for station $i$, $\\vec{X}_s=[x_s,y_s,z_s]$ the earthquake location, $T$ the travel-time function between the earthquake location and station location dependent on the subsurface velocity structure $V_H$.\n",
    "\n",
    "- The L1-norm misfit $r$ is \n",
    "$$ r = \\sum_{i=1}^n |t_o^i - t_p^i|,$$\n",
    "\n",
    "where $n$ represents the total number of station locations, $t_o^i$ the observed arrival time at station location $i$, and $t_p^i$ the predicted arrival time at station location $i$ given in the equation above.\n",
    "\n",
    "The observed time values are estimated using the homogeneous velocity model\n",
    "\n",
    "$$T_{V_H}^i = \\frac{\\sqrt{\\left(\\mathbf{X}_i  - \\mathbf{X}_s  \\right)^2}}{V_H},$$\n",
    "\n",
    "where $T_V^i$ represents the travel-time for station $i$ dependent on velocity structure $V_H$. Even for this simple case notice the non-linear nature in $T_V^i$ dependent on the source location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather data X, labels Y, define a model M(W) and loss function L(X, Y; W)\n",
    "- The synthetic data X is generated in the next cell (Xc and Xs) \n",
    "- The labels Y are the observed arrival times, calculated using the homogeneous velocity model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Defining the domain of interest: X = horizontal distance in km, Z r= depth, positive downwards in km\n",
    "Xmin,Xmax,Zmin,Zmax = 0,100,-2,10\n",
    "\n",
    "# ---- Number of seismic stations\n",
    "n  = 2000\n",
    "\n",
    "# ---- X\n",
    "Xc      = np.zeros((n,2));\n",
    "Xc[:,0] = np.random.uniform(low=Xmin, high=Xmax, size=(n))\n",
    "Xc[:,1] = 0  # Setting stations at surface \n",
    "\n",
    "# --- Define a random source location\n",
    "Xs =  [np.random.uniform(low=Xmin, high=Xmax),\n",
    "       np.random.uniform(low=0, high=Zmax)]\n",
    "\n",
    "print(f'\\nSource and stations defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Determining the observed travel-times for a single-phase ---\n",
    "Vh = 3.1 #km/s\n",
    "\n",
    "# The labels Y: the observational Travel-Times assuming origin time t0=0.0\n",
    "\n",
    "tt = np.sqrt(np.sum(np.square(Xc - Xs),axis=1))/Vh\n",
    "\n",
    "print(f'\\n {tt.shape} observational Travel-Times defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the data to torch tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors \n",
    "Xc_t = torch.from_numpy(Xc)\n",
    "Xs_t = torch.tensor(n*[Xs])\n",
    "\n",
    "#\n",
    "#------ PyTorch expects the I/P data to be in float32 by default. A numpy array is float64 (double) by default.\n",
    "Y = torch.from_numpy(tt).reshape(-1, 1).float()\n",
    "\n",
    "print(f'\\n Shapes of tensors: {Xc_t.shape}, {Xs_t.shape}, {Y.shape}')\n",
    "\n",
    "# Concatenate them together \n",
    "X = torch.cat((Xc_t, Xs_t), dim=1).float()\n",
    "\n",
    "print(f'  all concatenated in {X.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define the model\n",
    "* The **4** input features are the 2-D co-ordinates for a given station and the 2-D co-ordinates of the earthquake.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./modtt.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "1) Inherit the torch.nn.Module (include it in the class definition as class FCN(**torch.nn.Module**)\n",
    "-  more details: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "\n",
    "2) Define the model structure in the def __init__ function\n",
    "- A simple fully connected linear layer can be defined using the nn.Linear function. \n",
    "- The first layer are the **4** incoming features.\n",
    "- The second layer are the **8** outgoing features.\n",
    "\n",
    "3) Every **PyTorch class** needs to have a forward function: the forward pass of the model.\n",
    "- The different components defined in the model structure, with the **init function** are used for this forward function.\n",
    "- Note that we are passing the output of each component through non-linear functions wherever required using **F.Relu(self.fc1(x)))**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(torch.nn.Module): \n",
    "    def __init__(self):\n",
    "        super(FCN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(4, 8)\n",
    "        self.fc2 = nn.Linear(8, 1)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x \n",
    "\n",
    "print(f' Model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#------ Define the loss (misfit) function \n",
    "loss_fn = nn.L1Loss(reduction='mean')\n",
    "\n",
    "print(f' Misfit function defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize parameters W "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#------ The model parameters get initialized by creating an instance of the class \n",
    "model = FCN()\n",
    "#\n",
    "#------ Learning rate controls the rate of learning (how much should we react to the gradient signal)\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Optimizers basically optimize the gradient descent learning process. One of the most widely used optimizer is Adam's Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "print(f' Parameters **W** defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the loss L w.r.t those parameters\n",
    "\n",
    "### Update the parameters such that loss L is reduced\n",
    "\n",
    "### Keep doing this until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of epochs determine how many times do we want to go over the entire data \n",
    "# Learning stops once the maximum number of epochs are reached \n",
    "epochs = 10000 \n",
    "\n",
    "for i in range(epochs): \n",
    "    # Get output from the model (predicted arrival times for all stations)\n",
    "    y_pred = model(X) \n",
    "    \n",
    "    # This is step 3 - Calculating the loss \n",
    "    loss = loss_fn(y_pred, Y)\n",
    "        \n",
    "    if i % 500 == 0: \n",
    "        print(\"Loss\", loss.item())\n",
    "        \n",
    "    \n",
    "    # The optimizer by default accumulates gradient. \n",
    "    # So remember to call optimizer.zero_grad() as we only want to run the backward pass w.r.t the current gradient signal \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    # loss.backward() will calculate the gradient w.r.t all parameters \n",
    "    loss.backward()\n",
    "    # Optimizer.step() will perform the actual parameter update \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Actual values: {}\".format(Y[:100].squeeze()))\n",
    "#print(\"Predicted values: {}\".format(y_pred[:100].squeeze()))\n",
    "\n",
    "x = np.arange(0, len(Y))                                      #[:100]\n",
    "plt.plot(x, Y, label='Actual')                          #y_pred[:100]\n",
    "plt.plot(x, y_pred.detach().numpy(), label='Predicted') #\n",
    "# Customize the plot (optional)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Earthquake and Noise Signals from images\n",
    "1) Build a CNN classifier to classify between Earthquake Signals and Noise Signals (background noise). We will be using data from [Earthquake Detective](https://www.zooniverse.org/projects/vivitang/earthquake-detective).\n",
    "\n",
    "2) Classify between Earthquake and Noise using images alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from urllib.request import urlopen \n",
    "\n",
    "X_img = pickle.load(urlopen(\"https://www.dropbox.com/s/s3jy5cc6wcd5i9c/X_img?dl=1\"))\n",
    "Y = pickle.load(urlopen(\"https://www.dropbox.com/s/zvzemxjfodggbwq/Y?dl=1\"))\n",
    "\n",
    "#\n",
    "#------ Check how many images\n",
    "print(len(X_img), len(Y))\n",
    "\n",
    "print(X_img[0][0].shape)\n",
    "\n",
    "#\n",
    "#------ Plot an earthquake across three channels (BHZ, BHE, BHN)\n",
    "fig, axes = plt.subplots(3)\n",
    "\n",
    "axes[0].imshow(X_img[50][0], cmap='gray')\n",
    "axes[1].imshow(X_img[50][1], cmap='gray')\n",
    "axes[2].imshow(X_img[50][2], cmap='gray')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#------ Plot noise \n",
    "\n",
    "fig, axes = plt.subplots(3)\n",
    "\n",
    "axes[0].imshow(X_img[38][0], cmap='gray')\n",
    "axes[1].imshow(X_img[38][1], cmap='gray')\n",
    "axes[2].imshow(X_img[38][2], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide our data into train and test sets\n",
    "1) Randomly choose 80% of the data samples for training\n",
    "2) The remaining 20% are for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_img = np.array(X_img)\n",
    "Y = np.array(Y)\n",
    "\n",
    "X = torch.from_numpy(X_img).float()\n",
    "Y = torch.from_numpy(Y).long()\n",
    "\n",
    "n = X_img.shape[0]\n",
    "\n",
    "# Shuffle the values and divide into train and test \n",
    "permut = np.random.permutation(n)\n",
    "\n",
    "train_percent = 0.8 \n",
    "train_samples = int(train_percent * n)\n",
    "test_samples = n - train_samples \n",
    "print(\"Number of train samples {} and Number of test samples {}\".format(train_samples, test_samples))\n",
    "X_train, Y_train = X[permut[:train_samples], ], Y[permut[:train_samples]]\n",
    "X_test, Y_test = X[permut[train_samples:], ], Y[permut[train_samples: ]]\n",
    "#print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Structure for a CNN model\n",
    "Build a **Convolutional Neural Network** model\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./cnn.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "1) There is no fixed way to decide the values used while defining a model.\n",
    "   - In **self.conv1** we I/P **3 grayscale images** and O/P **8 filters (channels)** with kernel size = 5 and stride 2.\n",
    "   - This is form a **mostly trial and error** procedure.\n",
    "\n",
    "2) To determine the **number of features** going into a fully connected CNN network, follow the mathematical formula shown in the PyTorch documentation for each CNN module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=(5, 5), stride=2)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 32, kernel_size=(3, 3), stride=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
    "        self.fc1 = nn.Linear(3264, 512) \n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "    \n",
    "    def forward(self, x): \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # print(\"Shape after flattening\", x.shape)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=1)\n",
    "        \n",
    "        return x \n",
    "\n",
    "print(f' CNN model defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = CNN()\n",
    "# Run a single pass of the model \n",
    "#model(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "# Cross entropy loss is commonly used for classification tasks: \n",
    "# Link: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50 \n",
    "\n",
    "Lloss = []\n",
    "train = []\n",
    "test  = []\n",
    "\n",
    "for i in range(epochs): \n",
    "    y_pred = model(X_train) \n",
    "    \n",
    "    loss = loss_fn(y_pred, Y_train)\n",
    "    \n",
    "    if i % 5 == 0: \n",
    "        print(\"Loss\", loss.item())\n",
    "        Lloss.append(loss.item())\n",
    "        \n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        correct_mat = (predicted == Y_train).squeeze()\n",
    "        correct_count = torch.sum(correct_mat).item()\n",
    "        print(\"Train Accuracy: {}\".format(correct_count/len(predicted)))\n",
    "        train.append(correct_count/len(predicted))\n",
    "        \n",
    "        with torch.no_grad(): \n",
    "            y_pred_test = model(X_test)\n",
    "            _, predicted = torch.max(y_pred_test, 1)\n",
    "            correct_mat = (predicted == Y_test).squeeze()\n",
    "            correct_count = torch.sum(correct_mat).item()\n",
    "            print(\"Test Accuracy: {}\".format(correct_count/len(predicted)))\n",
    "            test.append(correct_count/len(predicted))\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "x = np.arange(0, len(Lloss))\n",
    "plt.plot(x, Lloss, label='Loss')\n",
    "plt.plot(x, train, label='Train')\n",
    "plt.plot(x, test, label='Test')\n",
    "# Customize the plot (optional)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
