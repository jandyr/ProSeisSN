{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HeMSvOAPfX5S"
   },
   "source": [
    "## ---\n",
    "# Unit11: Ambient Noise Tomography\n",
    "\n",
    "This notebook has the activities of the Course **ProSeisSN**. It deals with time series processing using a passive seismic dataset using [ObsPy](https://docs.obspy.org/).\n",
    "\n",
    "#### Dependencies: Obspy, Numpy, Matplotlib\n",
    "#### Reset the Jupyter notebook in order to run it again, press:\n",
    "***Kernel*** -> ***Restart & Clear Output***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== PyCWT: wavelet spectral analysis in Python ======================\n",
    "\"\"\"\n",
    "!pip install pycwt\n",
    "import pycwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "uzMJ-lGlkGuK",
    "outputId": "66c27f9e-fd21-4a54-ed6d-6a23b77ec540",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#------ Import Libraries\n",
    "import sys\n",
    "import os\n",
    "    \n",
    "#------ Work with the directory structure to include auxiliary codes\n",
    "print('\\n Local directory ==> ', os.getcwd())\n",
    "print('  - Contents: ', os.listdir(), '\\n')\n",
    "\n",
    "path = os.path.abspath(os.path.join('..'))\n",
    "if path not in sys.path:\n",
    "    sys.path.append(path+\"/CodePy\")\n",
    "\n",
    "%run ../CodePy/ImpMod.ipynb\n",
    "\n",
    "#------ Alter default matplotlib rcParams\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.dates as dates\n",
    "# Change the defaults of the runtime configuration settings in the global variable matplotlib.rcParams\n",
    "plt.rcParams['figure.figsize'] = 9, 5\n",
    "#plt.rcParams['lines.linewidth'] = 0.5\n",
    "plt.rcParams[\"figure.subplot.hspace\"] = (.9)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "#------ Magic commands\n",
    "%matplotlib inline\n",
    "%matplotlib widget\n",
    "#%pylab notebook\n",
    "%config Completer.use_jedi = False\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Read data files from TTB22 as SEG2. Create a new stream from the SEG2 stream\n",
    "- Read phone positions\n",
    "- Select and read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== READ PHONES LOCATIONS ======================\n",
    "\"\"\"\n",
    "#------ Read the phones cartesian locations\n",
    "#--- Reads the CSV file with (x, y)m locations\n",
    "ttb_loc = u.RGloc('../Data/'+'ttb_loc.dat')\n",
    "#------ Read the phones geographic locations\n",
    "#--- Reads the CSV file with (lat,lon) in degress locations\n",
    "ttb_gloc = u.RGloc('../Data/'+'ttb_gloc.dat')\n",
    "#\n",
    "#------ Plot gather in cartesian\n",
    "p.pgather(ttb_loc[:,1], ttb_loc[:,2], ttb_loc[:,0], coord='cartesian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== READ THE SEISMIC DATA LOCALLY ======================\n",
    "File hints:\n",
    "3710 and 3720 -> several events\n",
    "3740 -> 2 events\n",
    "3790 -> 1 event (6-9)s\n",
    "\"\"\"\n",
    "#------ Read the seismic data\n",
    "ent = str(np.random.choice(np.arange(3700, 3811, 10)))\n",
    "ent = input(f'   Enter a file number in [3695, 3810], rtn=random:\\n') or ent\n",
    "ent = ent.rstrip().split(' ')\n",
    "print(f\">> Read with data file {ent}\")\n",
    "ent = '../Data/ttb/'+ent[0]+'.dat'\n",
    "#\n",
    "#------- Read the data file as a SEG2 object.\n",
    "st     = read(ent)\n",
    "#\n",
    "#------- Print stream information\n",
    "dummy = float(st[-1].stats.seg2.RECEIVER_LOCATION)\n",
    "print(f\">> Gather acquired on {st[0].stats.starttime}, has {int(st[0].stats.npts)} data points.\")\n",
    "\"\"\"\n",
    "================= Create a new stream from the SEG2 stream ======================\n",
    "                         Retain a gather copy\n",
    "\"\"\"\n",
    "#------ Create a new stream from the SEG2 stream.\n",
    "#       1) Adds coordinates to gather. Stores a copy in gather0\n",
    "#       2) Gather baricenter = bcenter.\n",
    "gather, bcenter = u.creastrm(st, ttb_gloc)\n",
    "gather0 = gather.copy()\n",
    "#\n",
    "#--- Phone choice\n",
    "phone = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data processing\n",
    "- Filter data\n",
    "- Display the seismogram\n",
    "### Filter and look at seismogram and to the frequency content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\"\"\"\n",
    "================= Filter data and look at the frequency contents ======================\n",
    "                    Create a new stream from the SEG2 stream\n",
    "\"\"\"\n",
    "#\n",
    "#------- Remove mean and trend + filter the stream\n",
    "#--- Filter parameters: change them as you wish.\n",
    "MTparam = [ 1,   1,    'bp',  10.,   40.,   0,    0]\n",
    "# └─────> [dtr, line, ftype, Fmin, Fmax, taper, gain]\n",
    "#                                          └─> data will be windowed at trace normalization and spectral whitening\n",
    "ent = str(MTparam[3]) + ' ' + str(MTparam[4])\n",
    "ent = input(f'\\n Enter filter min and max frequencies (dflt = {MTparam[3]}, {MTparam[4]})') or ent\n",
    "ent = ent.rstrip().split(' ')\n",
    "MTparam[3], MTparam[4] = [float(dummy) for dummy in ent]\n",
    "#\n",
    "gather = u.otrstr(gather, MTparam)\n",
    "#\n",
    "#------- Check frequency contents to accept preprocessing\n",
    "#--- Pick up a random phone/trace\n",
    "phone = phone if phone is not None else np.random.randint(1, len(gather)+1)\n",
    "print(f' Random phone {phone} ')\n",
    "#--- Go to trace instead of phone: trace = phone -1\n",
    "phone = phone - 1\n",
    "#--- Relative time: nummpy array\n",
    "time = gather[phone].times(type=\"relative\")\n",
    "#--- Plot Trace+Spectrogram\n",
    "p.Pspect(time, gather[phone])\n",
    "#\n",
    "#------- Once filtering is accepted create a new backup for gather\n",
    "ent = input(f' Run this cell again (rtn= No, else plot Spectrogram)?: ') or False\n",
    "if not ent:\n",
    "    gather0 = gather.copy()\n",
    "    print(f' A new stream backup was created.')\n",
    "else:\n",
    "    gather = gather0.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down-sample the data\n",
    "- Down-sample the data to number of pints compatible with the upper limit of the bandpass filter\n",
    "- Reduce computational costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "gather = gather0.copy()\n",
    "\"\"\"\n",
    "================= Downsample stream by an integer factor ======================\n",
    "\"\"\"\n",
    "print(f'\\n>> Phone {phone+1} has {gather[phone].stats.npts} data points with a sampling rate of {gather[phone].stats.sampling_rate}Hz,')\n",
    "dummy =  u.divisors(int(gather[phone].stats.sampling_rate), MTparam[4])\n",
    "print(f'    this sampling rate can be lowered to the following integer values {dummy}Hz')\n",
    "ent = input(f'\\n<< Enter a new sampling rate from the above list:')\n",
    "ent = float( ent.rstrip().split(' ')[0] )\n",
    "\"\"\"\n",
    "Decimate (other possiblity is resample)\n",
    "1) Only every decimation_factor-th sample remains in the trace.\n",
    "2) Prior to decimation it is applyed a lowpass filter to prevente aliasing artifacts.\n",
    "3) To abort when\n",
    "           len(data) % decimation_factor != 0\n",
    "    set strict_length=True.\n",
    "\"\"\"\n",
    "#--- // is a floor division = integer floor. Sanity\n",
    "factor = int(gather[phone].stats.sampling_rate / ent)\n",
    "if gather[phone].stats.npts % factor != 0: raise ValueError(\"Decimation factor is not an integer.\")\n",
    "gather.decimate(factor=factor, strict_length=True)\n",
    "#--- Check on Fmax\n",
    "MTparam[4] = MTparam[4] if MTparam[4] <= ent else ent\n",
    "#\n",
    "print(f'\\n>> Phone {phone+1} has now {gather[phone].stats.npts} data points with a new sampling rate of {gather[phone].stats.sampling_rate}Hz.')\n",
    "print(f'    Resampled data is FIR low-pass filtered to prevent aliasing, with a decimation factor of  {factor}.')\n",
    "#\n",
    "#------- Check frequency contents of a trace to accept downsampling\n",
    "#--- Relative time: nummpy array\n",
    "time = gather[phone].times(type=\"relative\")\n",
    "#--- Plot Trace+Spectrogram\n",
    "p.Pspect(time, gather[phone])\n",
    "#\n",
    "#------- Once filtering is accepted create a new backup for gather\n",
    "ent = input(f' Run this cell again (rtn= No)?: ') or False\n",
    "if not ent:\n",
    "    gather0 = gather.copy()\n",
    "    print(f' A new stream backup was created.')\n",
    "else:\n",
    "    gather = gather0.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== Plot Seismogram ======================\n",
    "\"\"\"\n",
    "gather.plot(type='section',\n",
    "            scale=1.3, alpha=.7,\n",
    "            orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== Zoom in to select [t0, t1]======================\n",
    "\"\"\"\n",
    "#------ Zoom in the seismogram\n",
    "\n",
    "ent = input(f' Enter t0 and t1 to zoom: ')\n",
    "\n",
    "ent = ent.rstrip().split(' ')\n",
    "t0 = float(ent[0])\n",
    "t1 = float(ent[1])\n",
    "#\n",
    "dt = gather[0].stats.starttime\n",
    "gather.plot(type='section',\n",
    "            scale=1.3, alpha=.7,\n",
    "            starttime=dt+t0, endtime=dt+t1,\n",
    "            orientation='horizontal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== BEAMFORMING ======================\n",
    "\"\"\"\n",
    "dummy = UTCDateTime()\n",
    "# ---------- Use FK Analysis\n",
    "out, stime, etime = u.BeamFK(gather, [MTparam[3], MTparam[4]], ttb_gloc)\n",
    "print(\"\\n>> Total time in Beamforming: %f\\n\" % (UTCDateTime() - dummy))\n",
    "#---------- Change output\n",
    "t, rel_power, abs_power, baz, slow = out.T\n",
    "#--- Time\n",
    "T = np.linspace(stime, etime, num=len(out))\n",
    "#--- Semblance -> Fisher\n",
    "F = (len(gather)-1) * out[:, 1] / (1 - out[:, 1])\n",
    "#--- FK power\n",
    "FKp = out[:, 2]\n",
    "#--- baz\n",
    "#out[:, 3] = out[:, 3] % 360.\n",
    "baz = baz % 360.\n",
    "#--- Slowness -> Velocity\n",
    "V = 1.e3 / out[:,4]\n",
    "#------------- print\n",
    "sys.stdout.write('\\n')\n",
    "print(f'\\n>>  t   Fisher  FKpwr   baz(deg) vel(m/s)')    \n",
    "for i in range(len(out)):\n",
    "    print(f'   {round(T[i],2)}, {round(F[i],2)}, {round(FKp[i],2)}, {round(baz[i],2)}, {round(V[i],2)}')    \n",
    "#------------ Plot\n",
    "p.pltbaz(out, stime , etime)\n",
    "\"\"\"\n",
    "1) CLICK ON BLUE BAR TO EXPAND\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Is this velocity expected to be real?\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./rockVel.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "### Choose phone pairs\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./ttbg.png\" width=\"500\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== Local functions ======================\n",
    "\"\"\"\n",
    "#\n",
    "# ---------- Cuts data into time segments ----------\n",
    "\"\"\"\n",
    "Cuts continous noise data into user-defined segments, estimate statistics for each segment and keep timestamps for later use.\n",
    "  <Args>\n",
    "    tr        -> A trace, an 1-dimensional timeseries array\n",
    "    step      -> % overlapping (0, 1) between two sliding windows\n",
    "    cc_len    -> Segment length (sec) to cut trace data\n",
    "  <Returns>\n",
    "    trace_stdS: standard deviation of the noise amplitude of each segment\n",
    "    trace_madS: mad of the noise amplitude of each segment\n",
    "    dataS_t:    timestamps of each segment\n",
    "    dataS:      2D matrix of the segmented data\n",
    "\"\"\"\n",
    "def cut_trace(tr, cc_len, step):\n",
    "#\n",
    "#------ Trace stats and initialize return variables\n",
    "#--- sampling_rate\n",
    "    sps       = int(tr.stats.sampling_rate)\n",
    "#--- trace window length (sec)\n",
    "    tw        = round(tr.stats.endtime - tr.stats.starttime, 0)\n",
    "#--- Date and time of the first data sample given in UTC (default value is “1970-01-01T00:00:00.0Z”)\n",
    "    starttime = tr.stats.starttime - obspy.UTCDateTime(1970,1,1)\n",
    "#--- Overlap in sec\n",
    "    step = cc_len * step\n",
    "#--- Number of segments and of points in each segment\n",
    "    nseg      = int(np.floor((tw - cc_len)/step))    #tw/24*86400-cc_len\n",
    "    npts = int(cc_len * sps)\n",
    "#--- Copy data into array and check if data is shorter than the trim\n",
    "    data      = tr.data\n",
    "    if data.size < sps * nseg: raise ValueError(\"cc_len must be << trace.\")\n",
    "#--- Overlapping (sec) between two sliding windows\n",
    "    step      = np.ceil(cc_len * step).astype(int)\n",
    "#\n",
    "#------ initialize variables\n",
    "    dataS_t = []; dataS = []\n",
    "#--- Relevant arrays\n",
    "    dataS      = np.zeros(shape=(nseg, npts),dtype=np.float32)\n",
    "    trace_madS = np.zeros(nseg,dtype=np.float32)\n",
    "    trace_stdS = np.zeros(nseg,dtype=np.float32)\n",
    "    trace_madS = np.zeros(nseg,dtype=np.float32)\n",
    "    dataS_t    = np.zeros(nseg,dtype=np.float32)\n",
    "#\n",
    "#------ Statistic to detect segments that may be associated with a given event\n",
    "#--- median absolute deviation over all trace\n",
    "    all_madS = np.median(np.absolute(data - np.median(data)))\n",
    "#--- standard deviation over all noise window\n",
    "    all_stdS = np.std(data)\n",
    "    if all_madS==0 or all_stdS==0 or np.isnan(all_madS) or np.isnan(all_stdS):\n",
    "        print(\"Warn: madS or stdS == 0 for %s\" % tr)\n",
    "        return None, dataS_t, dataS\n",
    "#\n",
    "\n",
    "#    tw, cc_len, step 60.0 3.0 1.5\n",
    "#    data.size, sps, nseg, npts 3000 50 38 150\n",
    "    print('tw, cc_len, step', tw, cc_len, step)\n",
    "    \n",
    "    indx1 = 0\n",
    "    for iseg in range(nseg):\n",
    "        indx2 = indx1+npts\n",
    "\n",
    "        \n",
    "        print('data, iseg, indx1,indx2', data[indx1:indx2].size, iseg, indx1, indx2)\n",
    "\n",
    "        \n",
    "        dataS[iseg, :] = data[indx1:indx2]\n",
    "        trace_madS[iseg] = (np.max(np.abs(dataS[iseg]))/all_madS)\n",
    "        trace_stdS[iseg] = (np.max(np.abs(dataS[iseg]))/all_stdS)\n",
    "        dataS_t[iseg]    = starttime+step*iseg\n",
    "        indx1 = indx1+step*sps\n",
    "#\n",
    "#------ Data conditioning. It is assumed data is already demeaned and detrended\n",
    "    dataS = taper(dataS)\n",
    "#\n",
    "#------  Returns\n",
    "    return trace_stdS, trace_madS, dataS_t, dataS\n",
    "#\n",
    "# -------------- End of function   ---------------------\n",
    "#\n",
    "# ---------- Applies taper to time segments ----------\n",
    "\"\"\"\n",
    "Applies a cosine taper using obspy functions\n",
    "  <Args>\n",
    "    data -> An input data matrix\n",
    "  <Returns>\n",
    "    data -> A tapered data matrix\n",
    "\"\"\"\n",
    "def taper(data):\n",
    "#ndata = np.zeros(shape=data.shape,dtype=data.dtype)\n",
    "    if data.ndim == 1:\n",
    "        npts = data.shape[0]\n",
    "        # window length\n",
    "        if npts*0.05>20:wlen = 20\n",
    "        else:wlen = npts*0.05\n",
    "        # taper values\n",
    "        func = _get_function_from_entry_point('taper', 'hann')\n",
    "        if 2*wlen == npts:\n",
    "            taper_sides = func(2*wlen)\n",
    "        else:\n",
    "            taper_sides = func(2*wlen+1)\n",
    "        # taper window\n",
    "        win  = np.hstack((taper_sides[:wlen], np.ones(npts-2*wlen),taper_sides[len(taper_sides) - wlen:]))\n",
    "        data *= win\n",
    "    elif data.ndim == 2:\n",
    "        npts = data.shape[1]\n",
    "        # window length\n",
    "        if npts*0.05>20:wlen = 20\n",
    "        else:wlen = npts*0.05\n",
    "        # taper values\n",
    "        func = _get_function_from_entry_point('taper', 'hann')\n",
    "        if 2*wlen == npts:\n",
    "            taper_sides = func(2*wlen)\n",
    "        else:\n",
    "            taper_sides = func(2*wlen + 1)\n",
    "        # taper window\n",
    "        win  = np.hstack((taper_sides[:wlen], np.ones(npts-2*wlen),taper_sides[len(taper_sides) - wlen:]))\n",
    "        for ii in range(data.shape[0]):\n",
    "            data[ii] *= win\n",
    "#\n",
    "#------  Returns\n",
    "    return data\n",
    "#\n",
    "# -------------- End of function   ---------------------\n",
    "#\n",
    "# ---------- Spectral spectral normalization and whitening ----------\n",
    "\"\"\"\n",
    "Transforms to frequency domain, whitens the amplitude spectrum in the frequency domain between *freqmin* and *freqmax*,\n",
    " and returns the whitened fft.\n",
    "  <Args>\n",
    "    tr: A trace: 1-dimensional timeseries array\n",
    "    dt: The sampling space of the `data`\n",
    "    freqmin: The lower frequency bound\n",
    "    freqmax: The upper frequency bound\n",
    "    smooth_N: integer, it defines the half window length to smooth\n",
    "    freq_norm: whitening method between 'one-bit' and 'RMA'\n",
    "  <Returns>\n",
    "    FFTRawSign: The FFT (numpy.ndarray) of the whitened input trace in [freqmin, freqmax]\n",
    "\"\"\"\n",
    "#\n",
    "def whiten(data, delta, freqmin, freqmax, smooth_N, freq_norm):\n",
    "#\n",
    "#------ Speed up FFT by padding to optimal size for FFTPACK\n",
    "    if data.ndim == 1:\n",
    "        axis = 0\n",
    "    elif data.ndim == 2:\n",
    "        axis = 1\n",
    "#\n",
    "    Nfft = int(next_fast_len(int(data.shape[axis])))\n",
    "#\n",
    "#------ Apodization number to the left and to the right\n",
    "    Napod = 100\n",
    "#\n",
    "    Nfft = int(Nfft)\n",
    "    freqVec = scipy.fftpack.fftfreq(Nfft, d=delta)[:Nfft // 2]\n",
    "    J = np.where((freqVec >= freqmin) & (freqVec <= freqmax))[0]\n",
    "    low = J[0] - Napod\n",
    "    if low <= 0:\n",
    "        low = 1\n",
    "#\n",
    "    left = J[0]\n",
    "    right = J[-1]\n",
    "    high = J[-1] + Napod\n",
    "    if high > Nfft/2:\n",
    "        high = int(Nfft//2)\n",
    "#\n",
    "    FFTRawSign = scipy.fftpack.fft(data, Nfft,axis=axis)\n",
    "#\n",
    "#------  Left tapering:\n",
    "    if axis == 1:\n",
    "        FFTRawSign[:,0:low] *= 0\n",
    "        FFTRawSign[:,low:left] = np.cos(\n",
    "            np.linspace(np.pi / 2., np.pi, left - low)) ** 2 * np.exp(\n",
    "            1j * np.angle(FFTRawSign[:,low:left]))\n",
    "#\n",
    "#------ Pass band:\n",
    "        if freq_norm == 'phase_only':\n",
    "            FFTRawSign[:,left:right] = np.exp(1j * np.angle(FFTRawSign[:,left:right]))\n",
    "        elif freq_norm == 'rma':\n",
    "            for ii in range(data.shape[0]):\n",
    "                tave = moving_ave(np.abs(FFTRawSign[ii,left:right]),smooth_N)\n",
    "                FFTRawSign[ii,left:right] = FFTRawSign[ii,left:right]/tave\n",
    "#\n",
    "#------  Right tapering:\n",
    "        FFTRawSign[:,right:high] = np.cos(\n",
    "            np.linspace(0., np.pi / 2., high - right)) ** 2 * np.exp(\n",
    "            1j * np.angle(FFTRawSign[:,right:high]))\n",
    "        FFTRawSign[:,high:Nfft//2] *= 0\n",
    "#\n",
    "#------  Hermitian symmetry -> input is real\n",
    "        FFTRawSign[:,-(Nfft//2)+1:] = np.flip(np.conj(FFTRawSign[:,1:(Nfft//2)]),axis=axis)\n",
    "    else:\n",
    "        FFTRawSign[0:low] *= 0\n",
    "        FFTRawSign[low:left] = np.cos(\n",
    "            np.linspace(np.pi / 2., np.pi, left - low)) ** 2 * np.exp(\n",
    "            1j * np.angle(FFTRawSign[low:left]))\n",
    "#\n",
    "#------  Pass band:\n",
    "        if freq_norm == 'phase_only':\n",
    "            FFTRawSign[left:right] = np.exp(1j * np.angle(FFTRawSign[left:right]))\n",
    "        elif freq_norm == 'rma':\n",
    "            tave = moving_ave(np.abs(FFTRawSign[left:right]),smooth_N)\n",
    "            FFTRawSign[left:right] = FFTRawSign[left:right]/tave\n",
    "#\n",
    "#------  Right tapering:\n",
    "        FFTRawSign[right:high] = np.cos(\n",
    "            np.linspace(0., np.pi / 2., high - right)) ** 2 * np.exp(\n",
    "            1j * np.angle(FFTRawSign[right:high]))\n",
    "        FFTRawSign[high:Nfft//2] *= 0\n",
    "#\n",
    "#------  Hermitian symmetry  -> input is real\n",
    "        FFTRawSign[-(Nfft//2)+1:] = FFTRawSign[1:(Nfft//2)].conjugate()[::-1]\n",
    "#\n",
    "#------  Returns\n",
    "    return FFTRawSign\n",
    "#\n",
    "# -------------- End of function   ---------------------\n",
    "#\n",
    "# ---------- Wrapper for spectral normalization and whitening ----------\n",
    "\"\"\"\n",
    "Transforms to frequency domain, whitens the amplitude spectrum in the frequency domain between *freqmin* and *freqmax*,\n",
    " and returns the whitened fft.\n",
    "  <Args>\n",
    "    data: A 2D matrix of all segmented noise data\n",
    "    dt: The sampling distance in seconds\n",
    "    freqmin:   The lower frequency bound\n",
    "    freqmax:   The upper frequency bound\n",
    "    smooth_N:  Integer, it defines the half window length to smooth\n",
    "    time_norm: Time-domain normalization -> 'one_bit' or 'rma'\n",
    "    freq_norm: Whitening method -> 'one-bit' and 'RMA'\n",
    "  <Returns>\n",
    "    FFTRawSign: The FFT (numpy.ndarray) of the whitened input trace in [freqmin, freqmax]\n",
    "\"\"\"\n",
    "def noise_processing(data, delta, freqmin, freqmax, \n",
    "                     smooth_N = 100, time_norm = False, freq_norm = False):\n",
    "#\n",
    "    N = data.shape[0]\n",
    "#\n",
    "#------  Time normalization\n",
    "    if time_norm:\n",
    "#--- Sign normalization\n",
    "        if time_norm == 'one_bit':\n",
    "            white = np.sign(data)\n",
    "#--- Normalization over smoothed absolute average; running mean\n",
    "        elif time_norm == 'rma':\n",
    "            white = np.zeros(shape=data.shape,dtype=data.dtype)\n",
    "            for kkk in range(N):\n",
    "                white[kkk,:] = data[kkk,:]/moving_ave(np.abs(data[kkk,:]),smooth_N)\n",
    "#--- Don't normalize\n",
    "    else:\n",
    "        white = data\n",
    "#\n",
    "#------ whiten\n",
    "    if freq_norm:\n",
    "#--- Whiten and return FFT\n",
    "        White = whiten(white, delta, freqmin, freqmax, smooth_N, freq_norm)\n",
    "    else:\n",
    "#--- Return FFT\n",
    "        Nfft = int(next_fast_len(int(dataS.shape[1])))\n",
    "        White = scipy.fftpack.fft(white, Nfft, axis=1)\n",
    "#\n",
    "#------  Returns\n",
    "    return White\n",
    "#\n",
    "# -------------- End of function   ---------------------\n",
    "#\n",
    "# ---------- Running smooth average ----------\n",
    "\"\"\"\n",
    "This function does running smooth average for an array (use numba for performance)\n",
    "  <Args>\n",
    "    A: 1-D array of data to be smoothed\n",
    "    N: Defines the half window length to smooth (integer)\n",
    "  <Returns>\n",
    "    B: 1-D array with smoothed data\n",
    "\"\"\"\n",
    "def moving_ave(A,N):\n",
    "#\n",
    "    A = np.concatenate((A[:N],A,A[-N:]),axis=0)\n",
    "    B = np.zeros(A.shape,A.dtype)\n",
    "\n",
    "    tmp=0.\n",
    "    for pos in range(N,A.size-N):\n",
    "        # do summing only once\n",
    "        if pos==N:\n",
    "            for i in range(-N,N+1):\n",
    "                tmp+=A[pos+i]\n",
    "        else:\n",
    "            tmp=tmp-A[pos-N-1]+A[pos+N]\n",
    "        B[pos]=tmp/(2*N+1)\n",
    "        if B[pos]==0:\n",
    "            B[pos]=1\n",
    "    return B[N:-N]\n",
    "#\n",
    "# -------------- End of function   ---------------------\n",
    "#\n",
    "# ---------- Cross-correlation ----------\n",
    "\"\"\"\n",
    "Does the cross-correlation in freq domain. Keeps the sub-stacks of the cross-correlation if needed, taking advantage of\n",
    " the linear relationship of ifft. Stacking is performed in spectrum domain, reducing the total number of ifft.\n",
    " \n",
    "  <Args>\n",
    "    fft1:          Source station power (smoothed) spectral density\n",
    "    fft2:          Receiver station raw FFT spectrum\n",
    "    dt:            sampling rate (in s)\n",
    "    freqmin:       minimum frequency (Hz)\n",
    "    freqmax:       maximum frequency (Hz)\n",
    "    Nfft:          number of frequency points for ifft\n",
    "    maxlag:        maximum lags to keep in the cross correlation\n",
    "    dataS_t:       matrix of datetime object\n",
    "    method:        'xcorr' or \"coherency\"\n",
    "    substack:      sub-stack cross-correlationS or not\n",
    "    substack_len:  multiples of cc_len to stack ove\n",
    "    smoothspect_N: number of points to be smoothed for running-mean average (freq-domain, method==\"coherency\")\n",
    "  <Returns>\n",
    "    s_corr: 1D or 2D matrix of the averaged or sub-stacks of cross-correlation functions in time domain\n",
    "    t_corr: timestamp for each sub-stack or averaged function\n",
    "    n_corr: number of included segments for each sub-stack or averaged function\n",
    "\"\"\"\n",
    "#\n",
    "\n",
    "def correlate(fft1, fft2, dt, dataS_t,\n",
    "              freqmin, freqmax, maxlag,\n",
    "              method = 'xcorr', substack = False, substack_len  = None, smoothspect_N = None):\n",
    "#\n",
    "#------ check on fft1/2 dimensions: put a cap on them, assuning same diensions in both\n",
    "    if len(fft1) != len(fft2): raise ValueError(\"fft1 and fft2 must have the same length\")\n",
    "#--- nwin = number of segments in the 2D fft(i) matrix\n",
    "#\n",
    "#------ fft1 & fft2 are 2-D\n",
    "    if fft1.ndim == 2:\n",
    "        nwin  = fft1.shape[0]\n",
    "        Nfft = fft1.shape[1]\n",
    "        Nfft2 = Nfft//2\n",
    "#--- Cap second dimension to Nfft2\n",
    "        fft1 = fft1[:, :Nfft2]\n",
    "        fft2 = fft2[:, :Nfft2]\n",
    "#--- convert 2D array into 1D\n",
    "        corr = np.zeros(nwin*Nfft2,dtype=np.complex64)\n",
    "#        corr = fft1.reshape(fft1.size,)*fft2.reshape(fft2.size,)\n",
    "        corr = fft1.reshape(-1) * fft2.reshape(-1)\n",
    "#\n",
    "#------ fft1 & fft2 are 1-D\n",
    "    elif fft1.ndim == 1:\n",
    "        nwin  = 1\n",
    "        Nfft = fft1.shape[0]\n",
    "        Nfft2 = Nfft//2\n",
    "#--- Cap to Nfft2\n",
    "        fft1 = fft1[:Nfft2]\n",
    "        fft2 = fft2[:Nfft2]\n",
    "        corr = np.zeros(nwin*Nfft2,dtype=np.complex64)\n",
    "        corr = fft1 * fft2\n",
    "    else:\n",
    "        raise ValueError(\"fft must be either 1 or 2-dimensions\")\n",
    "#\n",
    "#------ method == coherency\n",
    "    if method == \"coherency\":\n",
    "        temp = moving_ave(np.abs(fft2.reshape(fft2.size,)),smoothspect_N)\n",
    "        corr /= temp\n",
    "    corr  = corr.reshape(nwin,Nfft2)            #it was Nfft2\n",
    "#\n",
    "#------ method != coherency\n",
    "    if substack:\n",
    "        if substack_len == cc_len:\n",
    "            # choose to keep all fft data for a day\n",
    "            s_corr = np.zeros(shape=(nwin,Nfft),dtype=np.float32)   # stacked correlation\n",
    "            ampmax = np.zeros(nwin,dtype=np.float32)\n",
    "            n_corr = np.zeros(nwin,dtype=np.int16)                  # number of correlations for each substack\n",
    "            t_corr = dataS_t                                        # timestamp\n",
    "            crap   = np.zeros(Nfft,dtype=np.complex64)\n",
    "            for i in range(nwin):\n",
    "                n_corr[i]= 1\n",
    "                crap[:Nfft2] = corr[i,:]            #it was Nfft2\n",
    "                crap[:Nfft2] = crap[:Nfft2]-np.mean(crap[:Nfft2])   # remove the mean in freq domain (spike at t=0). It was Nfft2\n",
    "                crap[-(Nfft2)+1:] = np.flip(np.conj(crap[1:(Nfft2)]),axis=0)            #it was Nfft2\n",
    "                crap[0]=complex(0,0)\n",
    "                s_corr[i,:] = np.real(np.fft.ifftshift(scipy.fftpack.ifft(crap, Nfft, axis=0)))\n",
    "\n",
    "            # remove abnormal data\n",
    "            ampmax = np.max(s_corr,axis=1)\n",
    "            tindx  = np.where( (ampmax<20*np.median(ampmax)) & (ampmax>0))[0]\n",
    "            s_corr = s_corr[tindx,:]\n",
    "            t_corr = t_corr[tindx]\n",
    "            n_corr = n_corr[tindx]\n",
    "\n",
    "        else:\n",
    "            # get time information\n",
    "            Ttotal = dataS_t[-1]-dataS_t[0]             # total duration of what we have now\n",
    "            tstart = dataS_t[0]\n",
    "\n",
    "            nstack = int(np.round(Ttotal/substack_len))\n",
    "            ampmax = np.zeros(nstack,dtype=np.float32)\n",
    "            s_corr = np.zeros(shape=(nstack,Nfft),dtype=np.float32)\n",
    "            n_corr = np.zeros(nstack,dtype=np.int)\n",
    "            t_corr = np.zeros(nstack,dtype=np.float)\n",
    "            crap   = np.zeros(Nfft,dtype=np.complex64)\n",
    "\n",
    "            for istack in range(nstack):\n",
    "                # find the indexes of all of the windows that start or end within\n",
    "                itime = np.where( (dataS_t >= tstart) & (dataS_t < tstart+substack_len) )[0]\n",
    "                if len(itime)==0:tstart+=substack_len;continue\n",
    "\n",
    "                crap[:Nfft2] = np.mean(corr[itime,:],axis=0)   # linear average of the correlation. It was Nfft2\n",
    "                crap[:Nfft2] = crap[:Nfft2]-np.mean(crap[:Nfft2])   # remove the mean in freq domain (spike at t=0). It was Nfft2\n",
    "                crap[-(Nfft2)+1:]=np.flip(np.conj(crap[1:(Nfft2)]),axis=0)            #it was Nfft2\n",
    "                crap[0]=complex(0,0)\n",
    "                s_corr[istack,:] = np.real(np.fft.ifftshift(scipy.fftpack.ifft(crap, Nfft, axis=0)))\n",
    "                n_corr[istack] = len(itime)               # number of windows stacks\n",
    "                t_corr[istack] = tstart                   # save the time stamps\n",
    "                tstart += substack_len\n",
    "                #print('correlation done and stacked at time %s' % str(t_corr[istack]))\n",
    "\n",
    "            # remove abnormal data\n",
    "            ampmax = np.max(s_corr,axis=1)\n",
    "            tindx  = np.where( (ampmax<20*np.median(ampmax)) & (ampmax>0))[0]\n",
    "            s_corr = s_corr[tindx,:]\n",
    "            t_corr = t_corr[tindx]\n",
    "            n_corr = n_corr[tindx]\n",
    "#\n",
    "#------ Not substack\n",
    "    else:\n",
    "        # average daily cross correlation functions\n",
    "        ampmax = np.max(corr,axis=1)\n",
    "        tindx  = np.where( (ampmax<20*np.median(ampmax)) & (ampmax>0))[0]\n",
    "        n_corr = nwin\n",
    "        s_corr = np.zeros(Nfft,dtype=np.float32)\n",
    "        t_corr = dataS_t[0]\n",
    "        crap   = np.zeros(Nfft,dtype=np.complex64)\n",
    "        crap[:Nfft2] = np.mean(corr[tindx],axis=0)            #it was Nfft2\n",
    "        crap[:Nfft2] = crap[:Nfft2]-np.mean(crap[:Nfft2],axis=0)            #it was Nfft2\n",
    "        crap[-(Nfft2)+1:]=np.flip(np.conj(crap[1:(Nfft2)]),axis=0)            #it was Nfft2\n",
    "        s_corr = np.real(np.fft.ifftshift(scipy.fftpack.ifft(crap, Nfft, axis=0)))\n",
    "\n",
    "    # trim the CCFs in [-maxlag maxlag]\n",
    "    t = np.arange(-Nfft2+1, Nfft2)*dt            #it was Nfft2\n",
    "    ind = np.where(np.abs(t) <= maxlag)[0]\n",
    "    if s_corr.ndim==1:\n",
    "        s_corr = s_corr[ind]\n",
    "    elif s_corr.ndim==2:\n",
    "        s_corr = s_corr[:,ind]\n",
    "#\n",
    "#------  Returns\n",
    "    return s_corr, t_corr, n_corr\n",
    "#\n",
    "# -------------- End of function   ---------------------\n",
    "\n",
    "\n",
    "#\n",
    "# ---------- extract the dispersion from the image ----------\n",
    "\"\"\"\n",
    "Takes the dispersion image from CWT as input, tracks the global maxinum on\n",
    "    the wavelet spectrum amplitude and extract the sections with continous and high quality data\n",
    "  <Args>\n",
    "    amp:   2D amplitude matrix of the wavelet spectrum\n",
    "    phase: 2D phase matrix of the wavelet spectrum\n",
    "    per:   period vector for the 2D matrix\n",
    "    vel:   vel vector of the 2D matrix\n",
    "  <Returns>\n",
    "    per:  central frequency of each wavelet scale with good data\n",
    "    gv:   group velocity vector at each frequency\n",
    "\"\"\"\n",
    "def extract_dispersion(amp,per,vel):\n",
    "#\n",
    "    maxgap = 5\n",
    "    nper = amp.shape[0]\n",
    "    gv   = np.zeros(nper,dtype=np.float32)\n",
    "    dvel = vel[1]-vel[0]\n",
    "#\n",
    "#------  find global maximum\n",
    "    for ii in range(nper):\n",
    "        maxvalue = np.max(amp[ii],axis=0)\n",
    "        indx = list(amp[ii]).index(maxvalue)\n",
    "        gv[ii] = vel[indx]\n",
    "#\n",
    "#------  check the continuous of the dispersion\n",
    "    for ii in range(1,nper-15):\n",
    "        # 15 is the minumum length needed for output\n",
    "        for jj in range(15):\n",
    "            if np.abs(gv[ii+jj]-gv[ii+1+jj])>maxgap*dvel:\n",
    "                gv[ii] = 0\n",
    "                break\n",
    "#\n",
    "#------  remove the bad ones\n",
    "    indx = np.where(gv>0)[0]\n",
    "#\n",
    "#------  Returns\n",
    "    return per[indx],gv[indx]\n",
    "#\n",
    "# -------------- End of function   ---------------------\n",
    "\n",
    "print(f'\\n>> Functions loaded.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== Phone pairs ======================\n",
    "\"\"\"\n",
    "print(f'\\n>> Choose pairs of the {len(gather)} using the polar angle of a rotating diameter.')\n",
    "ent = input(f'\\n<< Enter an angle step (rtn = 20 deg.):') or '20'\n",
    "ent = int( ent.rstrip().split(' ')[0] )\n",
    "matrix = u.pairs(ttb_loc, ent)\n",
    "print(f'ph1|ph2|     distance(m)    |angle(deg.)|')\n",
    "for row in matrix:\n",
    "    print(\" | \".join(map(str, row)))\n",
    "#\n",
    "#------ Choose a single pair\n",
    "ent = input(f'\\n<< Enter the 1st phone from above table:')\n",
    "ent = int( ent.rstrip().split(' ')[0] )\n",
    "i = next((i for i, row in enumerate(matrix) if row[0] == ent), -1)\n",
    "print(f'\\n>> Work with the pair [{matrix[i,0]}, {matrix[i,1]}], with a distance of {round(matrix[i,2],2)}m')\n",
    "#\n",
    "#------ Create an empty stream add the two traces\n",
    "distance = matrix[i,2]\n",
    "ntr1, ntr2 = matrix[i,:2] - 1\n",
    "st = Stream()\n",
    "st += gather[ntr1].copy()\n",
    "st += gather[ntr2].copy()\n",
    "print(f'Selected stream: {st}')\n",
    "#\n",
    "#------ Cut the trace to a window and taper it: 5% hann at both sides\n",
    "ent = input(f' Enter t0 and t1 to cut: ')\n",
    "ent = ent.rstrip().split(' ')\n",
    "t0 = float(ent[0])\n",
    "t1 = float(ent[1])\n",
    "#\n",
    "for tr in st:\n",
    "    tr.trim(starttime=tr.stats.starttime + t0, endtime=tr.stats.starttime + t1)\n",
    "    tr.taper(0.05)\n",
    "#\n",
    "print(f'Trimmed stream: {st}')\n",
    "#\n",
    "#------ plot\n",
    "dummy = st[0].times(type=\"relative\")         # time axis (s)\n",
    "fNy = st[0].stats.sampling_rate / 2.         # Nyquist\n",
    "for i in range(len(st)):\n",
    "    FtrTplt = np.fft.rfft(st[i].data)[1:]        # Discard DC component\n",
    "    if i == 0: freq = np.linspace(1, fNy, len(FtrTplt))   # Freq. axis (HZ). Discard f=0\n",
    "    p.pltTrSp( dummy, st[i].data, freq, abs(FtrTplt),\n",
    "              x1label='s', y1label='Ampl.', y1log=False, clr1 = 'k', \n",
    "              x2label='Hz', y2label='Spec.', y2log=True, clr2 = 'r' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Normalization and Spectral whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== Trace normalization and spectral whitening ======================\n",
    "NB: tr0W below will be next_fast_len(len(FtrTplt)) greater than len(FtrTplt)\n",
    "\"\"\"\n",
    "#\n",
    "#------- Work on first trace (source). Take the conjugate, so that the source spectrum is consistent with the receiver\n",
    "tr0W = noise_processing(st[0].data, st[0].stats.delta, MTparam[3], MTparam[4], \n",
    "                     smooth_N = 100, time_norm = 'one_bit', freq_norm = 'rma')\n",
    "tr0W = np.conjugate(tr0W)\n",
    "#\n",
    "#------- Plot whitened trace and its whitened spectrum\n",
    "#        NB: trace will not be tapered as it comes from the Fourier inverse.\n",
    "#--- Inverse fourier transform\n",
    "trw = np.fft.ifft(tr0W)           # (tr0W[1])\n",
    "#--- Discard small complex values left-overs after the inverse fourier transform.\n",
    "trw = np.real(trw)\n",
    "#--- Whitened trace has more points than the original one! Adjust time here.\n",
    "dummy = st[0].times(type=\"relative\")\n",
    "dummy = np.linspace(dummy[0], dummy[-1], num = len(trw))\n",
    "#--- The Nyquist is the original one\n",
    "freq = np.linspace(0, fNy, len(tr0W))   # Freq. axis (HZ). f=0 is not discarded here\n",
    "p.pltTrSp( dummy, trw, freq, abs(tr0W),\n",
    "              x1label='s', y1label='Ampl.', y1log=False, clr1 = 'k', \n",
    "              x2label='Hz', y2label='Spec.', y2log=True, clr2 = 'r' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== Trace normalization and spectral whitening ======================\n",
    "\"\"\"\n",
    "#\n",
    "#------- Work on second trace(the receiver.\n",
    "tr1W = noise_processing(st[1].data, st[1].stats.delta, MTparam[3], MTparam[4], \n",
    "                     smooth_N = 100, time_norm = 'one_bit', freq_norm = 'rma')\n",
    "#\n",
    "#------- Plot whitened trace and its whitened spectrum\n",
    "#--- Inverse fourier transform\n",
    "trw = np.fft.ifft(tr1W)           # (tr0W[1])\n",
    "#--- Discard small complex values left-overs after the inverse fourier transform.\n",
    "trw = np.real(trw)\n",
    "p.pltTrSp( dummy, trw, freq, abs(tr1W),\n",
    "              x1label='s', y1label='Ampl.', y1log=False, clr1 = 'k', \n",
    "              x2label='Hz', y2label='Spec.', y2log=True, clr2 = 'r' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== Cross correlation ======================\n",
    "\"\"\"\n",
    "#\n",
    "#------ Datetime object list to be used in correlation\n",
    "dataS_t = [st[0].stats.starttime]\n",
    "#\n",
    "#------ Enter max lag\n",
    "ent = st[0].stats.endtime - st[0].stats.starttime\n",
    "print(f'\\n>> Correlate trace {st[0].get_id()} with trace {st[1].get_id()}, with length={ent}s')\n",
    "ent = input(f'\\n<< Enter max lag time <={round(ent,1)} (rtn = {int(ent)}s):') or str(int(ent))\n",
    "ent = ent.rstrip().split(' ')\n",
    "maxlag = float(ent[0])\n",
    "#corr| Tstamp| #segm.           source|receiver   NB: t_corr is a timestamp object\n",
    "tdata, t_corr, n_corr = correlate(tr0W, tr1W, st[0].stats.delta, dataS_t,\n",
    "                                  MTparam[3], MTparam[4], maxlag,\n",
    "                                  method = 'xcorr')\n",
    "#\n",
    "#    dt = tr_source[0].stats.delta\n",
    "#--- A sequential time for the x-correlation\n",
    "tvec = np.linspace(-maxlag, maxlag, len(tdata))    #maxlag+dt\n",
    "#\n",
    "#------- plot correlation\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.plot(tvec,tdata)\n",
    "ax.set_xlabel(\"Time [s]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "#ax.set_title('Cross-Correlation Function Between %s and %s'%(ssta,rsta))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Dispersion Curves From Cross-Correlation Function\n",
    "A frequency-time image is produced using the wavelet transform, and then converted into a velocity-period image. The group velocity curve is then selected from the ridge on the velocity-frequency image. In this example, we will take the mean of both the positive and negative lags of the cross-correlation function. This is known as the symmetrical component of the cross-correlation function. Modified from NoisePy.\n",
    "\n",
    "The cross-correlations are symmetrized and turned into one-sided signals by averaging the positive and the negative lag parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "#\n",
    "# ---------- local script ----------\n",
    "\"\"\"\n",
    "Divides a vector at its center, flips the second part, and sums the two halves.\n",
    "\"\"\"\n",
    "def divSum(vector):\n",
    "    n = len(vector)\n",
    "    if n % 2 != 0:\n",
    "        vector = np.append(vector, 0.)\n",
    "        n += 1\n",
    "#\n",
    "    midpoint = int(n // 2)\n",
    "    first  = vector[:midpoint]\n",
    "    second = vector[midpoint:][::-1]  # Reverse the second half\n",
    "#\n",
    "    return first + second\n",
    "# -------------- End of function\n",
    "\"\"\"\n",
    "====================== Dispersion Curve ======================\n",
    "\"\"\"\n",
    "#\n",
    "#------ Set up parameters\n",
    "fmin = MTparam[3]; pmax = 1. / fmin\n",
    "fmax = MTparam[4]; pmin = 1. / fmax\n",
    "dt = st[0].stats.delta\n",
    "#--- wavelet transform parameters\n",
    "dj, s0, J, wvn = [1/12, -1, -1, 'morlet']\n",
    "#\n",
    "#------ The symmetrical correlation\n",
    "#--- number of points of the correalition\n",
    "#    len(tvec)=801; tvec = [-8,..., 8]  \n",
    "#npts = int(1/dt) * 2 * maxlag + 1              #npts = 511\n",
    "#indx = int(npts // 2)                          #indx = 400\n",
    "#--- len(tdata) = 511; tdata =[-5.8e-2,...,]\n",
    "#--- data = 0.5 * tdata[indx:] + 0.5 * np.flip(tdata[:indx+1], axis=0) --- data = tdata[indx:]\n",
    "#--- len(data) = 256; tdata =[-5.8e-2,...,]\n",
    "#\n",
    "ent = input(f'\\n<< Acausal (a), causal (c), or sum of the two parts (s) of x-corr, len={len(tdata)})?') or str(int(ent))\n",
    "ent = ent.rstrip().split(' ')\n",
    "if ent[0] not in ['a', 'c', 's']: raise ValueError(f\"'{ent}' not allowed\")\n",
    "indx = int(len(tdata) // 2)\n",
    "if ent == 'a':\n",
    "    data = tdata[:indx+1]\n",
    "    tvec = tvec[:indx+1]\n",
    "elif ent == 'c':\n",
    "    data = tdata[indx+1:]\n",
    "    tvec = tvec[indx+1:]\n",
    "else:\n",
    "    data = divSum(tdata) * 0.5\n",
    "    tvec = np.linspace(0, maxlag, len(data))\n",
    "#\n",
    "print(f'>> Work with the {ent[0]}-part with len={len(data)}')\n",
    "#\n",
    "#------ wavelet transformation\n",
    "cwt, sj, freq, coi, _, _ = pycwt.cwt(data, dt, dj, s0, J, wvn)\n",
    "#\n",
    "#------ filtering frequencies\n",
    "if (fmax < np.max(freq)) | (fmax <= fmin):\n",
    "    raise ValueError('Abort: frequency out of limits!')\n",
    "freq_ind = np.where((freq >= fmin) & (freq <= fmax))[0]\n",
    "cwt = cwt[freq_ind]\n",
    "freq = freq[freq_ind]\n",
    "#\n",
    "#------ use amplitude of the cwt\n",
    "period = 1 / freq\n",
    "rcwt = np.abs(cwt) ** 2\n",
    "#\n",
    "print(f'\\n>> Wavelet frequencies from {freq[0]} to {freq[-1]}Hz')\n",
    "#\n",
    "#------ interpolation to grids of freq-vel\n",
    "#--- velocity range for or disperion analysis (m/s)\n",
    "#    dt=0.02s and dist<30m -> vmax < 1000m/s\n",
    "vmin, vmax = [150., 1000.]\n",
    "vel  = np.linspace(vmin, vmax, len(period))\n",
    "timevec = distance / vel\n",
    "per  = np.linspace(pmin, pmax, len(period))\n",
    "#\n",
    "\n",
    "print(len(timevec))\n",
    "print(len(period))\n",
    "print(len(rcwt))\n",
    "\n",
    "\n",
    "X, Y = np.meshgrid(period, vel)\n",
    "Z = rcwt.reshape(len(X), len(Y))\n",
    "\n",
    "\n",
    "# Create the interpolator\n",
    "fc = RegularGridInterpolator((x, y), Z, method='linear', bounds_error=False, fill_value=np.nan)\n",
    "rcwt_new = fc(per, vel)\n",
    "#fc = scipy.interpolate.interp2d(distance / timevec, period, rcwt)\n",
    "#\n",
    "#------ normalization for each frequency\n",
    "for ii in range(len(per)):\n",
    "    rcwt_new[ii] /= np.max(rcwt_new[ii])\n",
    "\n",
    "\n",
    "\"\"\"     Repository\n",
    "how to separate the causal from the acausal parts of the cross-correlation?\n",
    "print(indx)\n",
    "print(len(tvec))\n",
    "pprint.pprint(tvec)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#\n",
    "#------ Measure the dispersion curve from the velocity-period map\n",
    "nper, gv = extract_dispersion(rcwt_new, per, vel)\n",
    "#\n",
    "#------ Find the first instance where the wavelength is greater than 1/3 of the inter-station distance\n",
    "lmbda = gv * nper\n",
    "bad_idx = np.where(lmbda > distance / 3)[0]\n",
    "#\n",
    "#------ Plot\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "im = ax.imshow(np.transpose(rcwt_new),cmap='gnuplot2',extent=[per[0],per[-1],vel[0],vel[-1]],\n",
    "           aspect='auto',origin='lower')\n",
    "ax.plot(nper,gv,'b--', linewidth=4.0)\n",
    "#--- Plot the first period where lambda is greater than the distance divided by 3\n",
    "ax.plot([nper[bad_idx[0]], nper[bad_idx[0]]], [vel[0], vel[-1]],\n",
    "        \"-\", color=\"black\", linewidth=4.0)\n",
    "ax.set_xlabel('Period [s]')\n",
    "ax.set_ylabel('U [m/s]')\n",
    "ax.set_title(\"Dispersion Curve\")\n",
    "cbar = plt.colorbar(im)\n",
    "cbar.set_label(\"Normalized Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## -----------------------------------------   Cut line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "print(f'>> Cut trace in segments to produce statistics. Enter parameters:')\n",
    "print(f'    cc_len    -> Segment length (sec) to cut trace data')\n",
    "print(f'    step      -> % overlapping (0, 1) between two contiguous sliding windows')\n",
    "#\n",
    "#------- I/P parameters\n",
    "CUTparam = '3. 0.5'\n",
    "# └─────>  [cc_len, step]\n",
    "ent = str(CUTparam)\n",
    "ent = input(f'\\n Enter cc_len, step (dflt = {CUTparam})') or ent[:]\n",
    "ent = ent.rstrip().split(' ')\n",
    "cc_len, step = [float(dummy) for dummy in ent]\n",
    "\n",
    "--------------------------------------------\n",
    "#\n",
    "#------ Cut trace 1 (source) into small segments and return statistics\n",
    "#  sdev   |     mad   |timestamps|segmented data|\n",
    "#                                             1st trace| segment(sec)|\n",
    "trace_stdS, trace_madS, dataS_t, dataS = cut_trace(st[0], cc_len, step)\n",
    "#\n",
    "#------ \n",
    "\n",
    "-------------------------------------------------------------\n",
    "dt   = st[0].stats.delta                  # sampling interval\n",
    "fNy  = 1. / (2.0 * dt)                    # Nyquist frequency\n",
    "-------------------------------------------------\n",
    "smooth_N  = 100          # number of points to be smoothed for running-mean average (time-domain)\n",
    "freq_norm   = 'rma'      # rma-> running mean average for frequency-domain normalization\n",
    "time_norm   = 'one_bit'  # no-> no time-domain normalization; other options 'rma' for running-mean and 'one_bit'\n",
    "\n",
    "------------------------------------------\n",
    "#- time = st[ntr1].times(type=\"relative\")\n",
    "#\n",
    "#------ Normalize and whiten\n",
    "for tr in st:\n",
    "#--- Taper the data with 10% Hanning\n",
    "    tr.taper(type = 'hann', max_percentage = 0.1)\n",
    "#--- Time normalization == 'one_bit' -> sign normalization\n",
    "    tr = np.sign(tr)\n",
    "#--- Whiten\n",
    "#    tr = u.whiten(tr, MTparam[3], MTparam[4])\n",
    "#    trw = whiten(tr, delta = dt, freqmin = MTparam[3], freqmax = MTparam[4], smooth_N  = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The direction-of-arrival (DOA). Choose an event with Beamforming\n",
    "- A wavefront arrives at the surface at an angle $i$ with the vertical. The wave propagates toward the surface with a velocity $v_{c}=\\frac{\\Delta s}{\\varDelta t}$, with a horizontal component $v_{h}=\\frac{\\Delta x}{\\varDelta t}$.\n",
    "\n",
    "- The horizontal slowness, $u_h$ is the inverse value of horizontal apparent velocity, $1/v_{h}$,\n",
    "$$\n",
    "u_{h}=\\frac{1}{v_{h}}=\\frac{\\sin i}{\\left|\\mathbf{v}_{c}\\right|},\n",
    "$$\n",
    "being related to: (a) the angle of incidence $i$, (b) the true velocity $v_c$ and (c) its azimuth with the North *toward* the epicenter; the **baz** ($\\theta$).\n",
    "$$\n",
    "\\boldsymbol{U}_0 = (\\frac{\\sin\\theta}{v_{h}},\\frac{\\cos\\theta}{v_{h}},\\frac{1}{v_{h}\\tan i})\n",
    "                 = \\frac{1}{v_{c}}(\\sin i\\sin\\theta,\\sin i\\cos\\theta,\\cos i)\n",
    "                 = u_{h}(\\sin\\theta,\\cos\\theta,\\frac{1}{\\tan i})\n",
    "                 = \\frac{1}{v_{c}}(\\sin i\\sin\\theta,\\sin i\\cos\\theta,\\cos i).\n",
    "$$\n",
    "\n",
    "- The seismic signals at each sensor can be time-shifted and summed to enhance the S/N ratio by a factor of $\\sqrt{N}$; the signals interfere constructively. The relative time shift of a given sensor $\\boldsymbol{r}_{i},\\,i=1,\\ldots,N$, relatively to the center of the array is \n",
    "$$\n",
    "\\tau_{i}=\\boldsymbol{r}_{i}.\\boldsymbol{u}.\n",
    "$$\n",
    "\n",
    "- The beamforming for the array is,\n",
    "$$\n",
    "\t\tb\\left(t\\right)=\\frac{1}{N}\\mathop{\\sum_{i=1}^{N}s_{i}\\left(t+\n",
    "\t\t\t\\mathbf{r}_{i}\\mathbf{\\cdot u}\\right)}=\\frac{1}{N}\\mathop{\\sum_{i=1}^{N}s_{i}\\left(t+\\tau_{i}\\right)}\n",
    "$$\n",
    "\n",
    "The ObsPy FK beamforming outputs the relative power, or semblance, and the absolute power, or FK power. Transforms the semblance $S$ to the Fisher or $F$-statistic as $F = (N-1) \\frac{S}{1-S}$. $F\\rightarrow1$ for white Gaussian noise, therefore if $F\\neq1$ means that there is some signal.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"./array1.png\" width=\"600\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ambient Noise Cross-correlation\n",
    "- Given two seismometers, $u_1$ and $u_2$, on the surface, will record ground motion as a function of time. Over long periods of time, the cross-correlation of ground motions is\n",
    "$$C_{1,2}\\left(\\tau\\right)=\\int u_{1}\\left(t\\right)\\,u_{2}\\left(t+\\tau\\right)dt$$\n",
    "\n",
    "- Data Preparation and inital processing\n",
    "Prepare waveform data from each station separately to accentuate broad-band ambient noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "====================== Trace correlation ======================\n",
    "\"\"\"\n",
    "print(f'\\n>> Correlate trace {ntr1} with {st[0].stats.npts} with trace {ntr2} with {st[1].stats.npts} points')\n",
    "ent = input(f'\\n<< Enter max lag time (rtn = 2s):') or '2'\n",
    "ent = float( ent.rstrip().split(' ')[0] )\n",
    "max_lagtime = ent\n",
    "#\n",
    "max_shift_num = int(np.round(max_lagtime*st[0].stats.sampling_rate))\n",
    "data1 = st[0].data\n",
    "data2 = st[1].data\n",
    "len1 = len(data1)\n",
    "len2 = len(data2)\n",
    "min_len = min(len1,len2)\n",
    "#\n",
    "cross_list = []\n",
    "for shift_num in np.arange(-max_shift_num,max_shift_num+1,1):\n",
    "    if shift_num<0:\n",
    "        correlate_value = np.correlate(data1[:min_len+shift_num],data2[-shift_num:min_len])\n",
    "        cross_list.append(correlate_value.ravel())\n",
    "    else:\n",
    "        correlate_value = np.correlate(data2[:min_len-shift_num],data1[shift_num:min_len])\n",
    "        cross_list.append(correlate_value.ravel())\n",
    "cross_list = np.array(cross_list)\n",
    "cross_list = cross_list/np.max(cross_list)\n",
    "#\n",
    "fs_new = st[0].stats.sampling_rate\n",
    "time = np.linspace(-max_lagtime,max_lagtime,int(2*max_lagtime*fs_new+1))\n",
    "#-------- \n",
    "indexmax = np.argmax(cross_list)\n",
    "travtime = time[indexmax]\n",
    "print(f'\\n>> Maximum lag = {travtime}s, corresponding to a velocity {np.round(distance/travtime, 2)}m/s')\n",
    "#\n",
    "plt.figure(figsize=(6, 2), dpi=180)\n",
    "plt.plot(time,cross_list, 'k-')\n",
    "#plt.plot(time, envelope, 'r-')\n",
    "plt.axvline(travtime, 0.85, 1, color='b', lw=3)\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"X-cor Coeff\")\n",
    "plt.xlim(-max_lagtime,max_lagtime)\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
